# fedrcl_p.yaml - Optimized for Kaggle GPU
#################### Default Setup ####################
wandb: False
seed: 42  # Fixed seed for reproducibility
enable_benchmark: True
use_amp: True  # Use automatic mixed precision for faster training
multiprocessing: False  # Kaggle has limited CPU cores
main_gpu: '0'
num_workers: 2  # Limit workers for Kaggle
pin_memory: True
refactoring: True
verbose: False
remark: "kaggle"

checkpoint_path: './checkpoints'
exp_name: "FedRCL-P-Kaggle"
output_model_path: 'fedrcl_p_model.pt'
save_freq: 10
batch_size: 16  # Reduced batch size for Kaggle GPU memory constraints
save_test: True

## Resume options
load_model_path: False
wandb_resume_id: False

###### wandb && logging ######
project: "FedRCL-Personalized"
log_dir: './checkpoints'

#################### Data Split ####################
split:
  mode: 'dirichlet'  # Options: 'iid', 'dirichlet', 'skew'
  alpha: 0.3  # Lower alpha = more non-IID
  class_per_client: 0  # Only for skew mode
  unbalanced: False
  overlap_ratio: 0.0
  create_client_dataset: False
  share_balanced_subset: true  # Share balanced data across clients
  samples_per_class: 10  # Reduced from 20 for Kaggle memory constraints
  samples_per_client: 5  # Reduced from 10 for Kaggle memory constraints

#################### Optimizer ####################
optimizer:
  name: sgd
  momentum: 0.9
  wd: 1e-4

#################### Eval ####################
eval:
  freq: 5  # Evaluate every 5 rounds
  batch_size: 16  # Reduced from 32 for Kaggle GPU memory
  max_samples: 500  # Further limit evaluation to 500 samples for Kaggle speed

#################### Trust-Based Client Filtering ####################
trust_filtering:
  enable: true
  threshold: 0.5
  min_trusted_clients: 1
  soft_filtering: true  # Use soft weighting instead of hard filtering

#################### Personalization Settings ####################
personalization:
  enable: true
  layers: 1  # Single personalized layer for memory efficiency
  knowledge_distillation: true
  kd_temperature: 2.0

#################### Adaptive Learning Rate ####################
adaptive_lr:
  enable: true
  base_lr: 0.01
  max_lr: 0.05

#################### Multi-Level Contrastive Learning ####################
contrastive:
  multi_level: true
  layer_weights: [0.3, 0.3, 0.4]  # Simplified weights for efficiency

#################### Memory Management ####################
memory:
  max_split_size_mb: 128  # Reduced from 256 for Kaggle
  empty_cache_freq: 1  # Empty CUDA cache every round for Kaggle
  gc_freq: 1  # Run garbage collection every round

#################### Training Settings ####################
trainer:
  global_rounds: 100  # Reduced from 200 for Kaggle time constraints
  local_epochs: 1  # Reduced from 3 for Kaggle time constraints
  min_clients_per_round: 2  # Reduced min clients for faster iterations
  participation_rate: 0.03  # Fraction of clients to select per round
  num_clients: 20  # Reduced from 100 for Kaggle memory constraints
  eval_every: 5  # Evaluate every 5 rounds
  local_lr: 0.01

#################### Analysis Settings ####################
analysis: True  # Enable analysis to track personalization benefits

#################### Default Configurations ####################
defaults:
  - _self_
  - trainer: personalized
  - client: fedrcl
  - evaler: base
  - dataset: cifar10  # Using CIFAR-10 dataset
  - server: personalized
  - model: personalized_resnet18
