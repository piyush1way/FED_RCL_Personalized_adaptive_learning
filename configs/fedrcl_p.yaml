# fedrcl_p.yaml
#################### Default Setup ####################
wandb: False
seed: 42  # Changed for better initialization
enable_benchmark: True
use_amp: True
multiprocessing: False
main_gpu: '0'
num_workers: 2
pin_memory: True
refactoring: True
verbose: False
remark: "personalized"

checkpoint_path: './checkpoints'
exp_name: "FedRCL-P"
output_model_path: 'fedrcl_p_model.pt'
save_freq: 10
batch_size: 32  # Reduced from 64 to save memory
save_test: True

## Resume options
load_model_path: False
wandb_resume_id: False

###### wandb && logging ######
project: "FedRCL-Personalized"
log_dir: './checkpoints'

#################### Data Split ####################
split:
  mode: 'dirichlet'  # Options: 'iid', 'dirichlet', 'skew'
  alpha: 0.3  # Lower alpha = more non-IID
  class_per_client: 0  # Only for skew mode
  unbalanced: False
  overlap_ratio: 0.0
  create_client_dataset: False
  share_balanced_subset: true  # Share balanced data across clients
  samples_per_class: 20  # Reduced from 50 to save memory
  samples_per_client: 10  # Reduced from 20 to save memory

#################### Optimizer ####################
optimizer:
  name: sgd
  momentum: 0.9
  wd: 1e-4

#################### Eval ####################
eval:
  freq: 5  # Evaluate every 5 rounds
  batch_size: 32  # Reduced from 128 to save memory
  max_samples: 1000  # Limit evaluation to 1000 samples to save memory

#################### Trust-Based Client Filtering ####################
trust_filtering:
  enable: true
  threshold: 0.5
  min_trusted_clients: 1
  soft_filtering: true  # Use soft weighting instead of hard filtering

#################### Personalization Settings ####################
personalization:
  enable: true
  layers: 1  # Reduced from 2 to 1 personalized layer to save memory and improve stability
  knowledge_distillation: true
  kd_temperature: 2.0  # Reduced from 3.0 to 2.0 for better calibration

#################### Adaptive Learning Rate ####################
adaptive_lr:
  enable: true
  base_lr: 0.01  # Increased from 0.001 to 0.01 for faster initial learning
  max_lr: 0.05  # Reduced from 0.1 to 0.05 for more stability

#################### Multi-Level Contrastive Learning ####################
contrastive:
  multi_level: true
  layer_weights: [0.2, 0.2, 0.3, 0.3]  # Simplified and reduced weight layers

#################### Memory Management ####################
memory:
  max_split_size_mb: 256  # Set PyTorch CUDA memory split size to avoid fragmentation
  empty_cache_freq: 5  # Empty CUDA cache every 5 rounds to prevent OOM

#################### Training Settings ####################
trainer:
  global_rounds: 200  # Total rounds of training
  local_epochs: 3  # Epochs per round (reduced from 5 to save computation)
  min_clients_per_round: 3  # Ensure at least 3 clients participate
  participation_rate: 0.03  # Fraction of clients to select per round
  num_clients: 100  # Total number of clients

#################### Analysis Settings ####################
analysis: True  # Enable analysis to track personalization benefits

#################### Default Configurations ####################
defaults:
  - _self_
  - trainer: personalized
  - client: personalized
  - evaler: base
  - dataset: cifar10  # Changed to CIFAR-10 to match error message
  - server: personalized
  - model: personalized_resnet18
