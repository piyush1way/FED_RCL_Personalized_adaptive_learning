# Trainer configuration for FedRCL
_target_: trainers.personalized_trainer.PersonalizedTrainer

# Training parameters
global_rounds: 1000
local_epochs: 5
local_bs: 32
local_lr: 0.01
momentum: 0.9
weight_decay: 1e-5

# Client selection
num_clients: 100
participation_rate: 0.03
eval_every: 1

# Personalization settings
personalization:
  enable: true
  adaptive_lr: true
  knowledge_distillation: true
  trust_filtering: true

# Adaptive freezing settings
adaptive_freezing:
  enable: true
  initial_freeze_ratio: 0.5
  decay_rate: 0.05

# Trust-based settings
trust_filtering:
  enable: true
  min_trust: 0.1
  max_trust: 1.0
  momentum: 0.9

# Learning rate settings
lr_scheduler:
  type: "cyclic"
  cycle_momentum: true
  base_lr: 0.001
  max_lr: 0.1
  step_size: 10

# Contrastive learning settings
contrastive:
  enable: true
  temperature: 0.07
  multi_level: true
  layer_weights: [0.2, 0.2, 0.2, 0.2, 0.2]

# Knowledge distillation settings
distillation:
  enable: true
  temperature: 3.0
  weight: 0.7
  feature_distillation: true

# EWC settings
ewc:
  enable: true
  lambda: 0.4
  importance: 5000 