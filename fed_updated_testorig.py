
# -*- coding: utf-8 -*-
"""IITH_submit_my_unet3dseg_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q0UQKIeQHv7myU1LDXVTQv5xs8hjPYH5
"""
#%%


# pip install segmentation_models_3D

# pip install tensorflow==2.12.0

# pip install tensorflow_addons

#%%
# importing libraries

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
# from http import serverq
from scipy import stats
import nibabel as nib
import cv2
import gc
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}
# import tensorflow_addons as tfa
import numpy as np
import pandas as pd
from glob import glob
import tensorflow as tf
from tqdm.auto import tqdm
# import keras.backend as K
import tensorflow.keras.backend as K
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
# import segmentation_models_3D as sm
from sklearn.utils import shuffle

#%%
print("tensorflow version: ",tf.__version__)



# Create a MirroredStrategy.
strategy = tf.distribute.MirroredStrategy()
print("Number of devices: {}".format(strategy.num_replicas_in_sync))

#%%
## Paths


# #Load dataset for client 1
# client_1_images_file = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/UPENN_GBM-PREPROCESSED/")
# client_1_masks_file = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/Labels-Preprocess-UPENN-GBM/")
# client_1_npy_images_path = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/UPENN_GBM-PREPROCESSED/", img_file) for img_file in client_1_images_file[:]]
# client_1_npy_masks_path = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/Labels-Preprocess-UPENN-GBM/", mas_file) for mas_file in client_1_masks_file[:]]
# # client_1_weight = "/raid/ai21resch01001/Fed_DA/model_saving/UPENN_weights_epoch501.h5"

# # Load dataset for client 2
# client_2_images_file = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/UCSF-PGDM-PREPROCESSED/")
# client_2_masks_file = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/Labels-Preprocess-UCSF-PGDM/")
# client_2_npy_images_path = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/UCSF-PGDM-PREPROCESSED/", img_file) for img_file in client_2_images_file[:]]
# client_2_npy_masks_path = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/Labels-Preprocess-UCSF-PGDM/", mas_file) for mas_file in client_2_masks_file[:]]
# # client_2_weight = "/raid/ai21resch01001/Fed_DA/model_saving/UCSF_PGDM_weights_epoch501.h5"


# # Load dataset for client 3
# client_3_images_file = os.listdir("/raid/ai21resch01001/Fed_DA/new_brain/preproessed_npy_data_task01/images_128/")
# client_3_masks_file = os.listdir("/raid/ai21resch01001/Fed_DA/new_brain/preproessed_npy_data_task01/masks_128/")
# client_3_npy_images_path = [os.path.join("/raid/ai21resch01001/Fed_DA/new_brain/preproessed_npy_data_task01/images_128/", img_file) for img_file in client_3_images_file[:]]
# client_3_npy_masks_path = [os.path.join("/raid/ai21resch01001/Fed_DA/new_brain/preproessed_npy_data_task01/masks_128/",mas_file) for mas_file in client_3_masks_file[:]]
# # client_3_weight = "/raid/ai21resch01001/Fed_DA/model_saving/msd_weights_epoch501.h5"


""" # UCSF-PGDM """
images_file_pgdm = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/UCSF-PGDM-64-Images/")
masks_file_pgdm = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/UCSF-PGDM-64-Masks/")
npy_images_path_pgdm = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/UCSF-PGDM-64-Images/", img_file) for img_file in images_file_pgdm[:]]
npy_masks_path_pgdm = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/UCSF-PGDM-64-Masks/",mas_file) for mas_file in masks_file_pgdm[:]]
pgdm_weight = "/raid/ai21resch01001/Fed_DA/model_saving/UCSF_PGDM_weights_epoch501.h5"

tstimg = np.load(npy_images_path_pgdm[0])
print("test image: ",tstimg.shape)

""" # UPENN_GBM """
images_file_gbm = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/GBM_64_Images/")
masks_file_gbm = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/GBM_64_Masks/")
npy_images_path_gbm = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/GBM_64_Images/", img_file) for img_file in images_file_gbm[:]]
npy_masks_path_gbm = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/GBM_64_Masks/",mas_file) for mas_file in masks_file_gbm[:]]
gbm_weight = "/raid/ai21resch01001/Fed_DA/model_saving/UPENN_weights_epoch501.h5"

tstimg = np.load(npy_images_path_gbm[0])
print("test image: ",tstimg.shape)

#MSD
images_file00 = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/MSD-64-Images/")
masks_file00 = os.listdir("/raid/ai21resch01001/Fed_DA/Datasets/MSD-64-Masks/")
npy_images_path00 = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/MSD-64-Images/", img_file) for img_file in images_file00[:]]
npy_masks_path00 = [os.path.join("/raid/ai21resch01001/Fed_DA/Datasets/MSD-64-Masks/",mas_file) for mas_file in masks_file00[:]]
msd_weight = "/raid/ai21resch01001/Fed_DA/model_saving/msd_weights_epoch501.h5"

tstimg = np.load(npy_images_path00[0])
print("test image: ",tstimg.shape)


# print('testing shape of 1 file')
# print(client_3_npy_images_path[0])

print('file loaded')
print('----------------------------------')


#%%
## loss functions

import numpy as np

def multivariate_cov(x,y):

  # find out covariance with respect  columns
  cov_mat = np.stack((x, y), axis = 0)
  return np.cov(cov_mat)[0][1]


x = [1.23, 2.12, 3.34, 4.5]
y = [2.56, 2.89, 3.76, 3.95]

multivariate_cov(x,y)

def multivariate_mean(x,y):
  xm = np.mean(np.array(x))
  ym = np.mean(np.array(y))
  return np.mean([xm,ym])

x=[1,2]
y=[3]
print(multivariate_mean(x,y))

# multivariate_norm(z_present)

# pip install tensorflow-probability==0.20.1
# pip install tensorflow-probability==0.10.1

# import tensorflow_probability as tfp

# x = tf.random.normal(shape=(100, 2, 3))
# y = tf.random.normal(shape=(100, 2, 3))

# # cov[i, j] is the sample covariance between x[:, i, j] and y[:, i, j].
# cov = tfp.stats.covariance(x, y, sample_axis=0, event_axis=None)

# # cov_matrix[i, m, n] is the sample covariance of x[:, i, m] and y[:, i, n]
# cov_matrix = tfp.stats.covariance(x, y, sample_axis=0, event_axis=-1)

# tf.convert_to_tensor(np.array(cov)[0].mean())
# # tf.math.reduce_mean(cov[0])

# from scipy.stats import multivariate_normal

def multivariate_mean(x,y): # y is serv, and x is z_prev ans z_present
  xm = tf.math.reduce_mean(x)#np.array(x))
  ym = tf.math.reduce_mean(y)#np.array(y))
  return tf.math.reduce_mean([xm,ym])
  # pass

def multivariate_cov(x,y): # y is serv, and x is z_prev ans z_present
  # find out covariance with respect  columns
  # cov_mat = tf.stack((x, y), axis = 0)
#   print('adfafa',x)
#   cov_mat = tfp.stats.covariance(x, y, sample_axis=0, event_axis=None)
  cov_matx = stats.Covariance.from_diagonal(x)
  cov_maty = stats.Covariance.from_diagonal(y)
#   print(cov_mat.shape)
  # print(cov_mat[0][0][0])
  # cov_mat = cov_mat
  # print(tf.linalg.eigvals(cov_mat[-1][-1][-1]))
  npcovx = np.array(cov_matx)[0][0]#[0]
  npcovy = np.array(cov_maty)[0][0]#[0]
  npcovS = np.array([[np.dot(npcovx,npcovy.T)]])
  print('sfdsf',np.linalg.eigvals(npcovS))
  # return tf.convert_to_tensor(np.array(cov_mat)[-1].mean())#tfp.stats.covariance(cov_mat)#cov_mat[0][1]
  return tf.convert_to_tensor(npcovS.mean())#tfp.stats.covariance(cov_mat)#cov_mat[0][1]
  # pass

def multivariate_norm(x,y): # x is z_present and z_prev and the mean & cov w.r.t. serv and each.
#   from scipy.stats import multivariate_normal
  x = K.flatten(x)
  y = K.flatten(y)
  x = np.array(x)
  y = np.array(y)
  print('cov:',multivariate_cov(x,y))

  norm = stats.multivariate_normal.pdf(x, mean=multivariate_mean(x,y), cov=multivariate_cov(x,y))
  return tf.convert_to_tensor(norm)

  # pass

# def multivariate_norm(x,y): # x is z_present and z_prev and the mean & cov w.r.t. serv and each.
#   x = K.flatten(x)
#   y = K.flatten(y)
#   xx = K.random_normal(x)
#   yy = K.random_normal(y)
#   mm = (xx + yy) / (xx * yy)
#   return mm

def triplate_mean_lr(prev, pres, serv,lr):
  # print(prev)
  # print(pres)
  # print(serv)
  xm = tf.math.reduce_mean(prev)
  ym = tf.math.reduce_mean(pres)
  zm = tf.math.reduce_mean(serv)
  return lr * (tf.math.divide(tf.math.reduce_mean([xm,ym,zm]),tf.math.add(xm,ym,zm)))
  # pass

# def contrastive_loss(z_prev,z_present,margin = 0.2):
#     dis = tf.sqrt(tf.reduce_sum(tf.square(z_present - z_prev)) + 1.0e-12)
#     # dis = tf.norm(z_present - z_prev, ord = "euclidean",axis = -1)
#     zeros = np.zeros(dis.shape)
#     loss = tf.math.square(tf.math.maximum(zeros, margin - dis))
#     loss = tf.math.reduce_mean(loss)
#     return loss

# def contrastive_loss(z_prev,z_present,z_serv,margin = 0.5, lr=1e-3):
#     #  z_prev = multivariate_norm(z_prev, z_serv)
#     #  z_present = multivariate_norm(z_present, z_serv)
#     # print(z_serv)
#      z_prev = tf.math.l2_normalize(z_prev)
#      z_present = tf.math.l2_normalize(z_present)
#      dis = tf.sqrt(tf.reduce_sum(tf.square(z_present - z_prev)))
#      dis = tf.sqrt(tf.math.maximum(dis, tf.keras.backend.epsilon())) # tf.keras.backend.epsilon()==1e-07
#      sqpres = tf.math.square(z_present)
#      sqMar = tf.math.square(tf.math.maximum(margin - z_present,0))
#      loss = tf.math.reduce_mean(z_prev*sqpres + (1-z_prev)*sqMar)
#      loss = triplate_mean_lr(z_prev,z_present,z_serv)
#      loss = loss - (loss*lr)
#      return loss

def contrastive_loss(z_prev,z_present,z_serv,temperature=0.5,margin = 0.2, lr=1e-3):
    #  z_prev = multivariate_norm(z_prev, z_serv)
    #  z_present = multivariate_norm(z_present, z_serv)
    # print(z_serv)
     z_prev = tf.math.l2_normalize(z_prev)
     z_present = tf.math.l2_normalize(z_present)
     dis = tf.sqrt(tf.reduce_sum(tf.square(z_present - z_prev)))
     dis = tf.sqrt(tf.math.maximum(dis, tf.keras.backend.epsilon())) # tf.keras.backend.epsilon()==1e-07
     sqpres = tf.math.square(z_present)
     sqMar = tf.math.square(tf.math.maximum(margin - z_present,0))
     loss = tf.math.reduce_mean(z_prev*sqpres + (1-z_prev)*sqMar)
     loss = loss - (loss*lr)
     lr = triplate_mean_lr(z_prev,z_present,z_serv,lr)
     srv = tf.sqrt(tf.reduce_sum(tf.square(z_serv)))
     #srv = tf.math.l2_normalize(srv)
     srv = tf.math.log(srv)
     return loss/srv

# def contrastive_loss(prev, pres, serv, tau=1):
#     cosine_similarity = tf.keras.losses.CosineSimilarity(axis=1)
#     # cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))
#     loss_con = - tf.math.log(tf.math.exp(cosine_similarity(pres, serv) / tau) / (tf.math.exp(cosine_similarity(pres, serv) / tau) + tf.math.exp(cosine_similarity(pres, prev) / tau)))
#     return loss_con

import math

"""# Final Code"""

# %%
def load_img(img_list):
    images=[]
    for i, image_name in enumerate(img_list):
        # print(i)
        # image = nib.load(image_name).get_fdata()
        image = np.load(image_name)
        images.append(image)
    # if images.shape
    images = np.asarray(images, dtype=np.float32)
    return(images)

# For nii.nz file
# def load_img(img_list):
#     images=[]
#     for i, image_name in enumerate(img_list):
#         # print(i)
#         image = nib.load(image_name).get_fdata()
#         # image = np.load(image_name)

#         # print(len(image.shape))
#         if len(image.shape) == 4:
#           image = image[:,:,:,:1]
#         elif len(image.shape) == 3:
#           image = np.expand_dims(image,axis=3)
#         else:
#           continue

#         images.append(image)

#     # if images.shape
#     images = np.asarray(images, dtype=np.float32)
#     return(images)


# npy_images_path00, npy_masks_path00 = shuffle(npy_images_path00,npy_masks_path00)
# npy_images_path00, npy_masks_path00 = shuffle(npy_images_path00,npy_masks_path00)
# npy_images_path00, npy_masks_path00 = shuffle(npy_images_path00,npy_masks_path00)

# test_image = nib.load(npy_images_path00[0]).get_fdata()
# test_image = np.load(npy_images_path00[0])
# print("test image: ",test_image.shape)


def split_train_test_data(images_path, masks_path, test_percentage=0.1):
    num_samples = len(images_path)
    num_test_samples = int(test_percentage * num_samples)

    test_images = images_path[:num_test_samples]
    test_masks = masks_path[:num_test_samples]

    train_images = images_path[num_test_samples:]
    train_masks = masks_path[num_test_samples:]

    return train_images, train_masks, test_images, test_masks

# Split data for each client
# client_1_train_images, client_1_train_masks, client_1_test_images, client_1_test_masks = split_train_test_data(client_1_npy_images_path, client_1_npy_masks_path)
# client_2_train_images, client_2_train_masks, client_2_test_images, client_2_test_masks = split_train_test_data(client_2_npy_images_path, client_2_npy_masks_path)
# client_3_train_images, client_3_train_masks, client_3_test_images, client_3_test_masks = split_train_test_data(client_3_npy_images_path, client_3_npy_masks_path)



"""## 3D-UNet"""

class UNET3DServer(tf.keras.Model):
    def __init__(self,classes):
        super(UNET3DServer,self).__init__()
        self.classes = classes
        # self.encoder_rep = None # s

    #helper functions
    def conv3D(self,x,
               filters,
               filter_size,
               activation = 'relu'):
        out = tf.keras.layers.Conv3D(filters,(filter_size,filter_size,filter_size),padding = "same",use_bias=True )(x)
        batch_norm_ = tf.keras.layers.BatchNormalization()(out)
        conv_batch_norm_act = tf.keras.layers.Activation('relu')(batch_norm_)
        return out

    def upsampling3D(self,x,
                     filters,
                     filter_size,
                     stride =2,
                     activation = 'relu'):
        up_out = tf.keras.layers.Conv3DTranspose(filters,(filter_size,filter_size,filter_size),strides = (stride,stride,stride),padding = 'same')(x)
        batch_norm_ = tf.keras.layers.BatchNormalization()(up_out)
        return batch_norm_

    def concatenate(self,x1,x2):
        concat = tf.keras.layers.concatenate([x1,x2])
        return concat

    def max_pool3D(self, x, filter_size,stride, name = None):
        out = tf.keras.layers.MaxPooling3D((filter_size,filter_size,filter_size),strides = stride, name = name)(x)
        return out

    def downconv(self,x,filters, name = None):
        s1 = self.conv3D(x,filters,3)
        s2 = self.conv3D(s1,filters,3)
        return s2

    def upconv(self,x,
               filters,skip_connection):
        e1 = self.upsampling3D(x,filters,6)
        # print("333333333333333333333333")
        # print("shape_x", x.shape)
        # print("shape_filters", filters)
        # print("shape1", e1.shape)
        # print("shape2", skip_connection.shape)
        # print("333333333333333333333333")
        concat = self.concatenate(e1,skip_connection)
        #layer2
        conv1 = self.conv3D(concat,filters,3)
        #layer 3
        conv2 = self.conv3D(conv1,filters,3)
        return conv2

    def call(self):
        #input
        #inputs = tf.keras.layers.Input(shape = (128,128,128,1))
        inputs = tf.keras.layers.Input(shape = (64,64,64,1))
        # inputs = tf.keras.layers.Input(shape = (240,240,155,1))
        #encoder
        # print(inputs.shape)
        d1 = self.downconv(inputs,32)
        # print(d1.shape)
        m1 = self.max_pool3D(d1,filter_size = 2,stride = 2)
        # print(m1.shape)
        d2 = self.downconv(m1,64)
        m2 = self.max_pool3D(d2,filter_size = 2,stride = 2)
        d3 = self.downconv(m2,128)
        m3 = self.max_pool3D(d3,filter_size = 2,stride = 2)
        d4 = self.downconv(m3,256)
        m4 = self.max_pool3D(d4,filter_size =2,stride=2, name = "layer_before_output")
        # encoder_output = m4
        # print(encoder_output.shape)

        #bottleneck
        bridge1 = self.conv3D(m4,1024,3,1)
        bridge2 = self.conv3D(bridge1,1024,3,1)
        # self.encoder_rep = bridge # s

        #decoder
        # print("bridge2_shape = ",bridge2.shape)
        # print("d4_shape = ",d4.shape)
        u1 = self.upconv(bridge2,256,d4)
        u2 = self.upconv(u1,128,d3)
        u3 = self.upconv(u2,64,d2)
        u4 = self.upconv(u3,32,d1)

        #1x1 output
        logits = tf.keras.layers.Conv3D(self.classes,(1,1,1),padding = "same")(u4)
        logits = tf.nn.sigmoid(logits)

        #model
        model = tf.keras.Model(inputs = [inputs],outputs = [bridge2, logits])

        # print(encoder.summary())
        # print(model.summary())

        # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        # client_model = tf.keras.Model(inputs = [inputs], outputs = [m4])
        return model

unet_s = UNET3DServer(1)


# class UNET3DClients(tf.keras.Model):
#     def __init__(self,classes):
#         super(UNET3DClients,self).__init__()
#         self.classes = classes

#     #helper functions
#     def conv3D(self,x,
#                filters,
#                filter_size,
#                activation = 'relu',name = None):
#         out = tf.keras.layers.Conv3D(filters,(filter_size,filter_size,filter_size),padding = "same",name =name,use_bias = True)(x)
#         batch_norm_ = tf.keras.layers.BatchNormalization()(out)
#         conv_batch_norm_act = tf.keras.layers.Activation('relu')(batch_norm_)
#         return out

#     def upsampling3D(self,x,
#                      filters,
#                      filter_size,
#                      stride =2,
#                      activation = 'relu'):
#         up_out = tf.keras.layers.Conv3DTranspose(filters,(filter_size,filter_size,filter_size),strides = (stride,stride,stride),padding = 'same')(x)
#         batch_norm_ = tf.keras.layers.BatchNormalization()(up_out)
#         return batch_norm_

#     def concatenate(self,x1,x2):
#         concat = tf.keras.layers.concatenate([x1,x2])
#         return concat

#     def max_pool3D(self, x, filter_size,stride, name = None):
#         out = tf.keras.layers.MaxPooling3D((filter_size,filter_size,filter_size),strides = stride, name = name)(x)
#         return out

#     def downconv(self,x,filters,name = None):
#         s1 = self.conv3D(x,filters,3, name =name )
#         s2 = self.conv3D(s1,filters,3, name = name + "0")
#         return s2

#     def call(self):
#         #input
#         # inputs = tf.keras.layers.Input(shape = (240,240,160,1))
#         # inputse = tf.keras.layers.Input(shape = (128,128,128,1))
#         inputse = tf.keras.layers.Input(shape = (64,64,64,1))
#         #encoder
#         d1e = self.downconv(inputse,32, name = "layer1")
#         m1e = self.max_pool3D(d1e,filter_size = 2,stride = 2)
#         d2e = self.downconv(m1e,64,  name = "layer2")
#         m2e = self.max_pool3D(d2e,filter_size = 2,stride = 2)
#         d3e = self.downconv(m2e,128, name = "layer3")
#         m3e = self.max_pool3D(d3e,filter_size = 2,stride = 2)
#         d4e = self.downconv(m3e,256, name = "layer4")
#         m4e = self.max_pool3D(d4e,filter_size =2,stride=2)

#         #model
#         modele = tf.keras.Model(inputs = [inputse],outputs = [m4e])
#         return modele
# client_unet = UNET3DClients(1)

#dice Coeff
def dice_coef(y_true, y_pred, smooth=1.0):

    # y_true_f = K.flatten(y_true)
    # y_pred_f = K.flatten(y_pred)
    # intersection = K.sum(y_true_f * y_pred_f)

    # return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth + intersection)
    smooth = 1.0
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)
    dice = (2.0 * intersection + smooth) / (union + smooth)
    return dice

def iou(y_true, y_pred):
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    iou_score = (intersection + tf.keras.backend.epsilon()) / (union + tf.keras.backend.epsilon())
    return iou_score

# Computing Precision 
def precision(y_true, y_pred):

    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())

    return precision

# Computing Sensitivity
def sensitivity(y_true, y_pred):

    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))

    return true_positives / (possible_positives + K.epsilon())

# Computing Specificity
def specificity(y_true, y_pred):

    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))

    return true_negatives / (possible_negatives + K.epsilon())

def combined_loss(y_true, y_pred, prev, pres, serv, temperature=0.5, dice_weight=0.5):
    dice_loss_value = 1-dice_coef(y_true, y_pred)
    con_loss_value = contrastive_loss(prev, pres, serv, temperature=temperature)
    total_loss = dice_weight * dice_loss_value + (1 - dice_weight) * con_loss_value
    return total_loss


#%%
def mini_batches_( X, Y, batch_size=64):
    """
    function to produce minibatches for training
    :param X: input placeholder
    :param Y: mask placeholder
    :param batch_size: size of each batch
    :return:
    minibatches for training
    """
    train_length = len(X)
    num_batches = int(np.floor(train_length / batch_size))
    batches = []
    for i in tqdm(range(num_batches)):
        batch_x = X[i * batch_size: i * batch_size + batch_size]
        batch_y = Y[i * batch_size:i * batch_size + batch_size]
        batch_x = load_img(batch_x)
        batch_y = load_img(batch_y)
        batch_x = batch_x.astype(np.float32)
        batch_y = batch_y.astype(np.float32)
        batches.append([batch_x, batch_y])
    return batches

def mini_batches_clients( X, batch_size=64):
    """
    function to produce minibatches for training
    :param X: input placeholder
    :param Y: mask placeholder
    :param batch_size: size of each batch
    :return:
    minibatches for training
    """
    train_length = len(X)
    num_batches = int(np.floor(train_length / batch_size))
    batches = []
    for i in tqdm(range(num_batches)):
        batch_x = X[i * batch_size: i * batch_size + batch_size]
        batch_x = load_img(batch_x)
        batch_x = batch_x.astype(np.float32)
        batches.append([batch_x])
    return batches


#%%

# setting parameters


# dice_loss = sm.losses.DiceLoss()
# focal_loss = sm.losses.BinaryFocalLoss()
# total_loss = dice_loss + (1 * focal_loss)



#load clients and data
# metrics = [sm.metrics.IOUScore(threshold=0.5),dice_coef,precision,sensitivity,specificity]
metrics = [iou,dice_coef,precision,sensitivity,specificity]

LR = 1e-5
optim = tf.keras.optimizers.Adam(LR)
bs=5# change it to 5 or 3 or 4
# no_img = 250

# %%
# Defning client and server

# tf.compat.v1.disable_eager_execution()

# print('-----------shape:',npy_images_path00[:1].shape)

length = len(npy_images_path_pgdm) + len(npy_images_path_gbm) + len(npy_images_path00)

# Client 1
print("client 1")
client_1 ={}
client_1["original_data"] = mini_batches_(npy_images_path_pgdm[:],npy_masks_path_pgdm[:],bs)
print("Client 1")
client_1["model"] = unet_s.call()
client_1["optimizer"] =tf.keras.optimizers.Adam(LR)
#client_1["past_model"] = tf.keras.models.load_model(pgdm_weight)
client_1["length_ratio"] = len(npy_images_path_pgdm)/length

#Client 2
print("client 2")
client_2 ={}
client_2["original_data"] = mini_batches_(npy_images_path_gbm[:],npy_images_path_gbm[:],bs)
print("client 2")
client_2["model"] = unet_s.call()
client_2["optimizer"] =tf.keras.optimizers.Adam(LR)
#client_2["past_model"] = tf.keras.models.load_model(gbm_weight)
client_2["length_ratio"] = len(npy_images_path_gbm)/length

# #Client 3
print("client 3")
client_3 ={}
client_3["original_data"] = mini_batches_(npy_images_path00[:],npy_masks_path00[:],bs)
print("client 3")
client_3["model"] = unet_s.call()
client_3["optimizer"] = tf.keras.optimizers.Adam(LR)
#client_3["past_model"] = tf.keras.models.load_model(msd_weight)
client_3["length_ratio"] = len(npy_images_path00)/length



# Server
Server=dict()
Server["model"]= unet_s.call()
print("server")
# Server["original_data"] = mini_batches_(npy_images_path00[:no_img],npy_masks_path00[:no_img],bs)
Server["optimizer"] = tf.keras.optimizers.Adam(LR)


# client_2 ={}
# client_2["original_data"] = mini_batches_(npy_images_path_gbm[:no_img],npy_images_path_gbm[:no_img],bs)
# print("client 2")
# client_2["model"] = unet_s.call()
# client_2["optimizer"] =tf.keras.optimizers.Adam(LR)


#Client 1
# client_1 ={}
# client_1["original_data"] = mini_batches_(client_1_train_images[:],client_1_train_masks[:],bs)
# client_1["model"] = unet_s().call()
# client_1["optimizer"] = tf.keras.optimizers.Adam(LR)
# # client_1["model"].load_weights(client_1_weight)
# # client_1["bs"] = bs

# #Client 2
# client_2 ={}
# client_2["original_data"] = mini_batches_(client_2_train_images[:],client_2_train_masks[:],bs)
# client_2["model"] = unet_s().call()
# client_2["optimizer"] = tf.keras.optimizers.Adam(LR)
# # client_2["model"].load_weights(client_2_weight)
# # client_2["bs"] = bs

# #Client 3
# client_3 ={}
# client_3["original_data"] = mini_batches_(client_3_train_images[:],client_3_train_masks[:],bs)
# client_3["model"] = unet_s().call()
# client_3["optimizer"] = tf.keras.optimizers.Adam(LR)
# # client_3["model"].load_weights(client_2_weight)
# # client_3["bs"] = bs


#Server
# Server=dict()
# Server["model"] = unet_s().call()
# Server["optimizer"] = tf.keras.optimizers.Adam(LR)

#%%

# import tensorflow as tf

# tf.compat.v1.enable_eager_execution()

# client_2["model"] = client_unet.call()
# client_1["model"] = client_unet.call()
# Server["model"] = unet_s.call()

#Losses
# dice_loss = sm.losses.DiceLoss()
# focal_loss = sm.losses.BinaryFocalLoss()
# total_loss = dice_loss + (1 * focal_loss)



'''
del npy_images_path
del npy_masks_path
del npy_brats20_images_paths
del npy_brats19_images_paths
'''


#%%

"""#Clients Training loop"""
print('test data:')
test_start = 120
inc = 10

gbm_testdata = mini_batches_(npy_images_path_gbm[test_start:test_start+inc], npy_masks_path_gbm[test_start:test_start+inc],10)
pdgm_testdata = mini_batches_(npy_images_path_pgdm[test_start:test_start+inc], npy_masks_path_pgdm[test_start:test_start+inc],10)
msd_testdata = mini_batches_(npy_images_path00[test_start:test_start+inc], npy_masks_path00[test_start:test_start+inc],10)

test_data_gen = mini_batches_(npy_images_path_gbm[test_start:test_start+inc]+npy_images_path_pgdm[test_start:test_start+inc]+npy_images_path00[test_start:test_start+inc], npy_masks_path_gbm[test_start:test_start+inc]+npy_masks_path_pgdm[test_start:test_start+inc]+npy_masks_path00[test_start:test_start+inc],10)

# test_data_gen = mini_batches_(client_1_test_images + client_2_test_images + client_3_test_images, client_1_test_masks + client_2_test_masks + client_3_test_masks,bs)

#%%
def sum_scaled_weights(scaled_weight_list,length_ratio):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    # print("------------")
    # print(len(scaled_weight_list[0][-1]))
    # print(scaled_weight_list[0])
    # print("------------")
    avg_grad = list()
    # get the average grad accross all client gradients
    for grad_list_tuple in zip(*scaled_weight_list):
          # pass
        # print("\n ------------")
        # print(len(grad_list_tuple))
        # norm = tf.math.l2_normalize(grad_list_tuple)
        # grad_list_tuple = grad_list_tuple
        layer_mean = tf.math.reduce_mean(grad_list_tuple, axis=0)#*length_ratio
        avg_grad.append(layer_mean)
    return avg_grad


# from matplotlib.pylab import *

def set_client_weights(server,client):
    #get server weights
    server_weights = []
    for slayer in server.layers:
        server_weights.append(server.get_layer(slayer.name).get_weights())
        if slayer.name == "layer_before_output":
            break

    #set client weights:
    print(nan in server_weights)
    for i, clayer in enumerate(client.layers):
        clayer.set_weights(server_weights[i])
    return client

def set_server_weights(server,client):
    client_weights = []
    for clayer in client.layers:
        client_weights.append(client.get_layer(clayer.name).get_weights())
    #set client weights:
    for i, slayer in enumerate(server.layers):
        slayer.set_weights(client_weights[i])
        if slayer.name == "layer_before_output":
            break
    return server

import gc
gc.collect()

# client_2["model"] = client_unet.call()
# client_1["model"] = client_unet.call()
# Server["model"] = unet_s.call()

# # def contrastive_loss(z_prev,z_present,margin = 0.2):
# #     dis = tf.sqrt(tf.reduce_sum(tf.square(z_present - z_prev)) + 1.0e-12)
# #     # dis = tf.norm(z_present - z_prev, ord = "euclidean",axis = -1)
# #     zeros = np.zeros(dis.shape)
# #     loss = tf.math.square(tf.math.maximum(zeros, margin - dis))
# #     loss = tf.math.reduce_mean(loss)
# #     return loss


# def contrastive_loss(z_prev,z_present,margin = 0.2):
#     dis = tf.sqrt(tf.reduce_sum(tf.square(z_present - z_prev)))
#     dis = tf.sqrt(tf.math.maximum(dis, tf.keras.backend.epsilon())) # tf.keras.backend.epsilon()==1e-07
#     sqpres = tf.math.square(z_present)
#     sqMar = tf.math.square(tf.math.maximum(margin - z_present,0))
#     loss = tf.math.reduce_mean(z_prev*sqpres + (1-z_prev)*sqMar)
#     return loss

# # def contrastive_loss(prev, pres, serv, tau=1):
# #     cosine_similarity = tf.keras.losses.CosineSimilarity(axis=1)
# #     # cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))
# #     loss_con = - tf.math.log(tf.math.exp(cosine_similarity(pres, serv) / tau) / (tf.math.exp(cosine_similarity(pres, serv) / tau) + tf.math.exp(cosine_similarity(pres, prev) / tau)))
# #     return loss_con

#define clients(test)
batch_size=bs
steps_per_epoch = 200//batch_size
val_steps_per_epoch = 84//batch_size
# iou = sm.metrics.IOUScore(threshold=0.5)
# iou = iou
epochs = 50
past_model = None
temperature = 0.5
mu = 1
num_comm = 3#5

# pre_weight = "/content/drive/MyDrive/VDI/saved_weights/weights_epoch50_final64.h5"
# checkpoint_path = "/content/drive/MyDrive/VDI/saved_weights/checkpoints/"

# dice_loss = sm.losses.DiceLoss()
# focal_loss = sm.losses.BinaryFocalLoss()
# total_loss = dice_loss + (1 * focal_loss)


clients = [client_1, client_2, client_3]
CE_loss = tf.keras.losses.CategoricalCrossentropy()



dice_coee=[]
contrastv_loss=[]
cl_pe_val = []
dc_pe_val = []
model_list = []

for t in range(num_comm):
#     c=0
    print('\ncommunication round: ',t+1,'\n')
#     i = 1
    local_weight_list = list()
    for epoch in range(epochs):

        print('\nEpoch ', epoch+1)
        c=0
        i=1


        client_loss = []
        for client in clients:

            c=c+1
            cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath="/kaggle/input/bts-pre-models/checkpoints/", save_weights_only=True, verbose=1)
            print(f'Training Client {i}')

            model = client['model']

#            model = client['past_model']

            # try:
            #   pre_weight = model.load_weights(pre_weight)
            #   model.set_weights(pre_weight)
            # except:

            # model.set_weights(Server['model'].get_weights())

            optimizer = client['optimizer']

            for batch_idx, (image, target) in enumerate(tqdm(client['original_data'])):

                # print(batch_idx)
                pro2, _ = Server['model'](image) # can change between client and server

                with tf.GradientTape() as tape:

                    pro1, output = model(image)

                    if 'past_model' not in client:
                        pro3 = tf.zeros_like(pro1)
                    else:
                        past_model = client['past_model']
                        pro3, past = past_model(image)

                    cl = contrastive_loss(pro3,pro1,pro2,0.5)
                    # print(cl)
                    loss2 = mu * cl

                    loss1 = 1-dice_coef(target, output)
                    dc = dice_coef(target, output)
                    # print('dice coeff:', dc)
                    # loss1 = sm.losses.DiceLoss(target, output)
                    # print('Loss1:', loss1,'\n type:',type(loss1))
                    loss = loss1 + loss2
                    client_loss.append(loss)
                    # print('dice loss:',loss1, '\ncontrastive loss:', cl)
                    dice_coee.append(dc)
                    contrastv_loss.append(cl)


                grads = tape.gradient(loss, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_variables))
            # printing epoch wise
            dc_val_pe = sum(dice_coee)/len(dice_coee)
            cl_val_pe = sum(contrastv_loss)/len(contrastv_loss)

            print('------------------')
            print('epoch dice coeff mean:',dc_val_pe)
            print('epoch contrastive loss mean:',cl_val_pe)
            print('------------------')

            dc_pe_val.append(dc_val_pe)
            cl_pe_val.append(cl_val_pe)

            dice_coee.clear()
            contrastv_loss.clear()

            # client['past_model'] = model
            client['model'] = model
            local_weight_list.append(client['model'].get_weights())


            model.save("/raid/ai21resch01001/Fed_DA/new_pretrained_models/"+ str(c) +".h5")
            # model.save_weights("new_weights_client_"+ str(i) +".h5")

            i += 1

            # train server
            print('\nTraining server\n')
            server_model = Server['model']
            CE_loss = tf.keras.losses.CategoricalCrossentropy()
            optimizer = Server['optimizer']
            # server_model.set_weights(server_model.load(pre_weight))
            average_weight = sum_scaled_weights(local_weight_list,client["length_ratio"])
            server_model.set_weights(average_weight)
            client['model'] = server_model
            # local_weight_list.clear()


            if epoch == epochs-1:
                #sd
                print("yes")

                "--------------------------------------------------------------------------------------------------------"
                "pdgm data test"

                dice = []
                pre = []
                batch_loss=[]
                se=[]
                spe = []
                io = []

                for batch_idx, (images, masks) in enumerate(tqdm(pdgm_testdata)):
                    _, logits = model(images)
                    logits, predictions = model(images)
                    pro2, _ = model(images)
                    pro1, _ = model(images)
                    pro3 = tf.zeros_like(pro1)

                    cl = contrastive_loss(pro3,pro1,pro2,0.5)
                    # print(cl)
                    loss2 = mu * cl

                    loss1 = 1-dice_coef(masks, predictions)
                    dc = dice_coef(masks, predictions)
                    loss = loss1 + loss2
                    batch_loss.append(loss)
                    dice.append(dice_coef(masks,logits))
                    pre.append(precision(masks,logits))
                    se.append(sensitivity(masks,logits))
                    spe.append(specificity(masks,logits))
                    io.append(iou(masks,logits))
                batch_loss = np.array(batch_loss).mean()
                dice = np.array(dice).mean()
                pre = np.array(pre).mean()
                se =  np.array(se).mean()
                spe =  np.array(spe).mean()
                io =  np.array(io).mean()

                print('PDGM test results @ client per epoch:')
                print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))

#                 dice[dice].clear()
#                 pre.clear()
#                 batch_loss.clear()
#                 se.clear()
#                 spe.clear()
#                 io.clear()

                "--------------------------------------------------------------------------------------------------------"
                "gbm data test"

                dice = []
                pre = []
                batch_loss=[]
                se=[]
                spe = []
                io = []

                for batch_idx, (images, masks) in enumerate(tqdm(gbm_testdata)):
                    _, logits = model(images)
                    logits, predictions = model(images)
                    pro2, _ = model(images)
                    pro1, _ = model(images)
                    pro3 = tf.zeros_like(pro1)

                    cl = contrastive_loss(pro3,pro1,pro2,0.5)
                    # print(cl)
                    loss2 = mu * cl

                    loss1 = 1-dice_coef(masks, predictions)
                    dc = dice_coef(masks, predictions)
                    loss = loss1 + loss2
                    batch_loss.append(loss)
                    dice.append(dice_coef(masks,logits))
                    pre.append(precision(masks,logits))
                    se.append(sensitivity(masks,logits))
                    spe.append(specificity(masks,logits))
                    io.append(iou(masks,logits))
                batch_loss = np.array(batch_loss).mean()
                dice = np.array(dice).mean()
                pre = np.array(pre).mean()
                se =  np.array(se).mean()
                spe =  np.array(spe).mean()
                io =  np.array(io).mean()

                print('GBM test results @ client per epoch:')
                print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))

#                 dice.clear()
#                 pre.clear()
#                 batch_loss.clear()
#                 se.clear()
#                 spe.clear()
#                 io.clear()

                "--------------------------------------------------------------------------------------------------------"
                "msd data test"

                dice = []
                pre = []
                batch_loss=[]
                se=[]
                spe = []
                io = []

                for batch_idx, (images, masks) in enumerate(tqdm(msd_testdata)):
                    _, logits = model(images)
                    logits, predictions = model(images)
                    pro2, _ = model(images)
                    pro1, _ = model(images)
                    pro3 = tf.zeros_like(pro1)

                    cl = contrastive_loss(pro3,pro1,pro2,0.5)
                    # print(cl)
                    loss2 = mu * cl

                    loss1 = 1-dice_coef(masks, predictions)
                    dc = dice_coef(masks, predictions)
                    loss = loss1 + loss2
                    batch_loss.append(loss)
                    dice.append(dice_coef(masks,logits))
                    pre.append(precision(masks,logits))
                    se.append(sensitivity(masks,logits))
                    spe.append(specificity(masks,logits))
                    io.append(iou(masks,logits))
                batch_loss = np.array(batch_loss).mean()
                dice = np.array(dice).mean()
                pre = np.array(pre).mean()
                se =  np.array(se).mean()
                spe =  np.array(spe).mean()
                io =  np.array(io).mean()

                print('MSD test results @ client per epoch:')
                print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))
#                 dice.clear()
#                 pre.clear()
#                 batch_loss.clear()
#                 se.clear()
#                 spe.clear()
#                 io.clear()


                "--------------------------------------------------------------------------------------------------------"
                "all merged data test"

                dice = []
                pre = []
                batch_loss=[]
                se=[]
                spe = []
                io = []

                # servmodel = Server["model"]

                for batch_idx, (images, masks) in enumerate(tqdm(test_data_gen)):
                    _, logits = model(images)
                    logits, predictions = model(images)
                    pro2, _ = model(images)
                    pro1, _ = model(images)
                    pro3 = tf.zeros_like(pro1)

                    cl = contrastive_loss(pro3,pro1,pro2,0.5)
                    # print(cl)
                    loss2 = mu * cl

                    loss1 = 1-dice_coef(masks, predictions)
                    dc = dice_coef(masks, predictions)
                    loss = loss1 + loss2
                    print('=========================>')
                    batch_loss.append(loss)
                    dice.append(dice_coef(masks,logits))
                    pre.append(precision(masks,logits))
                    se.append(sensitivity(masks,logits))
                    spe.append(specificity(masks,logits))
                    io.append(iou(masks,logits))
                batch_loss = np.array(batch_loss).mean()
                dice = np.array(dice).mean()
                pre = np.array(pre).mean()
                se =  np.array(se).mean()
                spe =  np.array(spe).mean()
                io =  np.array(io).mean()

                print('ALL MERGED test results @ client per epoch:')
                print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))
#                 dice.clear()
#                 pre.clear()
#                 batch_loss.clear()
#                 se.clear()
#                 spe.clear()
#                 io.clear()


                    # # train server
                    # print('\nTraining server\n')
                    # server_model = Server['model']
                    # CE_loss = tf.keras.losses.CategoricalCrossentropy()
                    # optimizer = Server['optimizer']
                    # # server_model.set_weights(server_model.load(pre_weight))
                    # average_weight = sum_scaled_weights(local_weight_list,client["length_ratio"])
                    # server_model.set_weights(average_weight)
                    # client['model'] = server_model

        local_weight_list.clear()

    # train server
    print('\nTraining server\n')
    server_model = Server['model']
    optimizer = Server['optimizer']
    # average_weight = sum_scaled_weights(local_weight_list,client["length_ratio"])
    # average_weight = scaled_weights(client['model'].get_weights(),client["length_ratio"])
    # server_model.set_weights(average_weight)
    # client['model'] = server_model

    print("----------------------------------------teseting after all epochs---------------------------------------------------------------")
    "--------------------------------------------------------------------------------------------------------"
    "pdgm data test"

    dice = []
    pre = []
    batch_loss=[]
    se=[]
    spe = []
    io = []

    for batch_idx, (images, masks) in enumerate(tqdm(pdgm_testdata)):
        _, logits = server_model(images)
        logits, predictions = model(images)
        pro2, _ = model(images)
        pro1, _ = model(images)
        pro3 = tf.zeros_like(pro1)

        cl = contrastive_loss(pro3,pro1,pro2,0.5)
        # print(cl)
        loss2 = mu * cl

        loss1 = 1-dice_coef(masks, predictions)
        dc = dice_coef(masks, predictions)
        loss = loss1 + loss2
        batch_loss.append(loss)
        dice.append(dice_coef(masks,logits))
        pre.append(precision(masks,logits))
        se.append(sensitivity(masks,logits))
        spe.append(specificity(masks,logits))
        io.append(iou(masks,logits))
    batch_loss = np.array(batch_loss).mean()
    dice = np.array(dice).mean()
    pre = np.array(pre).mean()
    se =  np.array(se).mean()
    spe =  np.array(spe).mean()
    io =  np.array(io).mean()

    print('PDGM test results @ client per epoch:')
    print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))


    "--------------------------------------------------------------------------------------------------------"
    "gbm data test"

    dice = []
    pre = []
    batch_loss=[]
    se=[]
    spe = []
    io = []

    for batch_idx, (images, masks) in enumerate(tqdm(gbm_testdata)):
        _, logits = server_model(images)
        logits, predictions = model(images)
        pro2, _ = model(images)
        pro1, _ = model(images)
        pro3 = tf.zeros_like(pro1)

        cl = contrastive_loss(pro3,pro1,pro2,0.5)
        # print(cl)
        loss2 = mu * cl

        loss1 = 1-dice_coef(masks, predictions)
        dc = dice_coef(masks, predictions)
        loss = loss1 + loss2
        batch_loss.append(loss)
        dice.append(dice_coef(masks,logits))
        pre.append(precision(masks,logits))
        se.append(sensitivity(masks,logits))
        spe.append(specificity(masks,logits))
        io.append(iou(masks,logits))
    batch_loss = np.array(batch_loss).mean()
    dice = np.array(dice).mean()
    pre = np.array(pre).mean()
    se =  np.array(se).mean()
    spe =  np.array(spe).mean()
    io =  np.array(io).mean()

    print('GBM test results @ client per epoch:')
    print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))


    "--------------------------------------------------------------------------------------------------------"
    "msd data test"

    dice = []
    pre = []
    batch_loss=[]
    se=[]
    spe = []
    io = []

    for batch_idx, (images, masks) in enumerate(tqdm(msd_testdata)):
        _, logits = server_model(images)
        logits, predictions = model(images)
        pro2, _ = model(images)
        pro1, _ = model(images)
        pro3 = tf.zeros_like(pro1)

        cl = contrastive_loss(pro3,pro1,pro2,0.5)
        # print(cl)
        loss2 = mu * cl

        loss1 = 1-dice_coef(masks, predictions)
        dc = dice_coef(masks, predictions)
        loss = loss1 + loss2
        batch_loss.append(loss)
        dice.append(dice_coef(masks,logits))
        pre.append(precision(masks,logits))
        se.append(sensitivity(masks,logits))
        spe.append(specificity(masks,logits))
        io.append(iou(masks,logits))
    batch_loss = np.array(batch_loss).mean()
    dice = np.array(dice).mean()
    pre = np.array(pre).mean()
    se =  np.array(se).mean()
    spe =  np.array(spe).mean()
    io =  np.array(io).mean()

    print('MSD test results @ client per epoch:')
    print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))


        # # train server
        # print('\nTraining server\n')
        # server_model = Server['model']
        # CE_loss = tf.keras.losses.CategoricalCrossentropy()
        # optimizer = Server['optimizer']
        # # server_model.set_weights(server_model.load(pre_weight))
        # average_weight = sum_scaled_weights(local_weight_list,client["length_ratio"])
        # server_model.set_weights(average_weight)
        # client['model'] = server_model


    client_loss = np.array(client_loss).mean()
    # print('------------------')
    print('\nClient loss: ', client_loss,'\n')
    print('overall per epoch dice coeff mean:',sum(dc_pe_val)/len(dc_pe_val))
    print('overall per epoch contrastive loss mean:',sum(cl_pe_val)/len(cl_pe_val))
    # print('------------------')

    # CE_loss = tf.keras.losses.CategoricalCrossentropy()
    print('\nTraining server\n')
    server_model = Server['model']
    optimizer = Server['optimizer']
    # average_weight = sum_scaled_weights(local_weight_list,client["length_ratio"])
    # server_model.set_weights(average_weight)
    # model = server_model

    dice = []
    pre = []
    batch_loss=[]
    se=[]
    spe = []
    io = []

    srv_model = Server["model"]

    for batch_idx, (images, masks) in enumerate(tqdm(test_data_gen)):
        _, logits = srv_model(images)
        logits, predictions = model(images)
        pro2, _ = model(images)
        pro1, _ = model(images)
        pro3 = tf.zeros_like(pro1)

        cl = contrastive_loss(pro3,pro1,pro2,0.5)
        # print(cl)
        loss2 = mu * cl

        loss1 = 1-dice_coef(masks, predictions)
        dc = dice_coef(masks, predictions)
        loss = loss1 + loss2
        print('^^^^^^^^^^^^^^^^^^^^^^^666666')
        #print(total_loss)
        batch_loss.append(loss)
        #metrics
        dice.append(dice_coef(masks,logits))
        pre.append(precision(masks,logits))
        se.append(sensitivity(masks,logits))
        spe.append(specificity(masks,logits))
        io.append(iou(masks,logits))
    batch_loss = np.array(batch_loss).mean()
    dice = np.array(dice).mean()
    pre = np.array(pre).mean()
    se =  np.array(se).mean()
    spe =  np.array(spe).mean()
    io =  np.array(io).mean()

    print('test results @ server:')
    print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))

#     dice.clear()
#     pre.clear()
#     batch_loss.clear()
#     se.clear()
#     spe.clear()
#     io.clear()


print("-------------------testing at the end--------------------")
# CE_loss = tf.keras.losses.CategoricalCrossentropy()

dice = []
pre = []
batch_loss=[]
se=[]
spe = []
io = []

model_s = Server["model"]

for batch_idx, (images, masks) in enumerate(tqdm(test_data_gen)):
    _, logits = model_s(images)
    logits, predictions = model(images)
    pro2, _ = model(images)
    pro1, _ = model(images)
    pro3 = tf.zeros_like(pro1)

    cl = contrastive_loss(pro3,pro1,pro2,0.5)
    # print(cl)
    loss2 = mu * cl

    loss1 = 1-dice_coef(masks, predictions)
    dc = dice_coef(masks, predictions)
    loss = loss1 + loss2
    print('^^^^^^^^^^^^^^^^^^^^^^^666666')
    # #print(total_loss)
    batch_loss.append(loss)
    #metrics
    dice.append(dice_coef(masks,logits))
    pre.append(precision(masks,logits))
    se.append(sensitivity(masks,logits))
    spe.append(specificity(masks,logits))
    io.append(iou(masks,logits))
batch_loss = np.array(batch_loss).mean()
dice = np.array(dice).mean()
pre = np.array(pre).mean()
se =  np.array(se).mean()
spe =  np.array(spe).mean()
io =  np.array(io).mean()

print('test results:')
print("Loss: {} , Dice Coeff: {}\n\n, Precision: {} Sensitivity: {} \n\n Specificity: {} , IOU: {}".format(batch_loss,dice,pre,se,spe,io))

print('DONE!!!')
